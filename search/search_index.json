{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"easyocr/","title":"EasyOCR","text":""},{"location":"easyocr/#what-os-easyocr","title":"What os EasyOCR","text":"<p>EasyOCR is a Python computer language Optical Character Recognition (OCR) module that is both flexible and easy to use. OCR technology is useful for a variety of tasks, including data entry automation and image analysis. It enables computers to identify and extract text from photographs or scanned documents.</p> <p>EasyOCR stands out for its dedication to making OCR implementation easier for developers. It\u2019s made to be user-friendly even for people with no background in OCR or computer vision. Multiple language support, pre-trained text detection and identification models, and a focus on speed and efficiency in word recognition inside images are all provided by the library.</p>"},{"location":"easyocr/#supported-languages-by-easyocr","title":"Supported Languages By EasyOCR","text":"<p>EasyOCR is currently supporting 80+ languages with more languages in development. Supported languages with the code name are listed below:</p> <p>Abaza (abq), Adyghe (ady), Afrikaans (af), Angika (ang), Arabic (ar), Assamese (as), Avar (ava), Azerbaijani (az), Belarusian (be), Bulgarian (bg) , Bihari (bh), Bhojpuri (bho), Bengali (bn), Bosnian (bs), Simplified Chinese (ch_sim), Traditional Chinese (ch_tra), Chechen (che), Czech (cs), Welsh (cy), Danish (da), German (de), English (en), Spanish (es), Estonian (et), Persian (fa), Finnish (fi), French (fr), Irish (ga), Goan Konkani (gom), Hindi (hi), Croatian (hr), Hungarian (hu), Indonesian (id), Ingush (inh), Icelandic (is), Italian (it), Japanese (ja), Kabardian (kbd), Kannada (kn), Korean (ko), Kurdish (ku), Latin (la), Lak (lbe), Lezghian (lez), Lithuanian (lt), Latvian (lv), Magahi (mah), Maithili (mai), Maori (mi), Mongolian (mn), Marathi (mr), Malay (ms), Maltese (mt), Nepali (ne), Newari (new), Dutch (nl), Norwegian (no), Occitan (oc), Pali (pi), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Serbian (cyrillic) (rs_cyrillic), Serbian(latin) (rs_latin), Nagpuri (sck), Slovak (sk), Slovenian (sl), Albanian (sq), Swedish (sv), Swahili (sw), Tamil (ta), Tabassaran (tab), Telugu (te), Thai (th), Tajik (tjk), Tagalog (tl), Turkish (tr), Uyghur (ug), Ukranian (uk), Urdu (ur), Uzbek (uz), Vietnamese (vi)</p>"},{"location":"easyocr/#how-to-use-easyocr","title":"How to use EasyOCR","text":""},{"location":"easyocr/#part-of-command-prompt","title":"Part of Command Prompt","text":"<p>Set up your python environment before the Installation.</p>"},{"location":"easyocr/#install-library","title":"Install library","text":"<p>Defualt is only English.</p> <pre><code>pip install easyocr\n</code></pre>"},{"location":"easyocr/#install-library-for-thai-language","title":"Install library for Thai language","text":"<pre><code>pip install easyocr[Thai]\n</code></pre>"},{"location":"easyocr/#part-of-code-editor","title":"Part of code editor","text":"<p>like VScode or Jupyter notebook</p>"},{"location":"easyocr/#import-library","title":"Import library","text":"<pre><code>import cv2\nimport easyocr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom glob import glob\n\nplt.style.use('ggplot')\n</code></pre>"},{"location":"easyocr/#input-observation","title":"Input Observation","text":"<pre><code># dataset path\ntest_imgs = glob('dataset/*')\n\nimage = test_imgs[9]\n\nimg=cv2.imread(image)\n\nplt.axis(\"off\")\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()\n</code></pre>"},{"location":"easyocr/#text-extraction-by-using-easyocr","title":"Text Extraction by using EasyOCR","text":"<pre><code># Extract text\nreader = easyocr.Reader(['th','en']) # specify the language  \nres = reader.readtext(image)\n\nfor (bbox, text, prob) in res:\n    (top_left, top_right, bottom_right, bottom_left) = bbox \n    print(f'Text: {text}, Probability: {prob}')\n</code></pre>"},{"location":"easyocr/#display-result","title":"Display result","text":"<pre><code># Plot image and its annotations\nimg = plt.imread(image)\nfig, ax = plt.subplots(figsize=(8, 8))\nax.imshow(img)\n\nfor (bbox, text, prob) in res:\n    (top_left, top_right, bottom_right, bottom_left) = bbox \n    # Draw bounding box\n    rect = plt.Rectangle(top_left, width=(bottom_right[0] - top_left[0]), \n                             height=(bottom_right[1] - top_left[1]), \n                             linewidth=2, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n\n    # Annotate with text \n    ax.text(top_left[0], top_left[1], f'{text}', va='baseline', color='blue', fontweight = 'heavy', fontfamily = 'Tahoma', fontsize = 'large')\n\nax.set_title('EASYOCR Result')\nplt.axis('off')\nplt.show()\n\npd.DataFrame(res, columns=['bbox', 'text', 'conf'])\n</code></pre>"},{"location":"easyocr/#if-you-want-to-extract-text-in-mutiple-images-files-follow-this-nbviewer","title":"If you want to extract text in mutiple images files, Follow this nbviewer \u26a0\ufe0f","text":""},{"location":"get_started/","title":"Get Started","text":""},{"location":"get_started/#in-anaconda-prompt-or-the-terminal","title":"In Anaconda Prompt or the Terminal","text":"<pre><code>cd your_directory\n</code></pre>"},{"location":"get_started/#creating-new-virtual-environment","title":"Creating new virtual environment","text":"<p>Create new folder name: \"sign_project\" on your directory. Put your own environment name instead of \"envName\" or you can you this name.</p> <pre><code>mkdir sign_project\ncd sign_project\npython -m venv envName\n</code></pre>"},{"location":"get_started/#activate-your-environment","title":"Activate your environment","text":"<pre><code>envName\\Scripts\\activate.bat\n</code></pre>"},{"location":"get_started/#using-git-clone-to-set-up-the-model-tools","title":"Using Git clone to set up the model tools","text":"<p>how to set up git click &gt;&gt;</p> <pre><code>git clone https://github.com/DreamPTK/yolov5.git\n</code></pre>"},{"location":"get_started/#open-visual-studio-code-vscode","title":"Open Visual Studio Code (VScode)","text":"<pre><code>code .\n</code></pre>"},{"location":"get_started/#in-visual-studio-code-vscode","title":"In Visual Studio Code (VScode)","text":"<p>Open <code>detect.ipynb</code> on <code>yolov5</code> folder and</p> <p>Open New Terminal in Vscode (command Prompt)</p> <pre><code>cd yolov5\npip install -qr requirements.txt  \n</code></pre>"},{"location":"get_started/#import-module-and-set-up-tools","title":"Import module and set up tools","text":"<p>In <code>detect.ipynb</code></p> <pre><code>import os\nimport utils\ndisplay = utils.notebook_init()\n</code></pre>"},{"location":"get_started/#detection-sign-image","title":"Detection Sign image","text":"<p>The <code>detect.py</code> run ThTaxSign model on all images source of <code>data/images</code> folder and save the result to <code>output/exp</code> folders.</p> <p>This case study have two images source on <code>data/images</code> folder.</p> <p><pre><code># use ThTaxSign model to detect \n!python detect.py --img 640 --weights ThTaxSigns/data/models.pt --conf 0.3  --source ThTaxSigns/data/images --project ThTaxSigns/data/output --save-crop\n</code></pre> <code>--img</code>: inference size (height, width)</p> <p><code>--weights</code>: model path or triton URL</p> <p><code>--conf</code>: confidence threshold</p> <p><code>--source</code>: file/dir/URL/glob/screen/webcam (source of the image)</p> <p><code>--project</code>: save results to project/name</p> <p><code>--save-crop</code>: save cropped prediction boxes</p> <p>Other option you can read more !!!</p>"},{"location":"get_started/#display-output","title":"Display output","text":"<pre><code># Display image\n\nimport glob\nfrom IPython.display import Image, display\n\nfor imageName in glob.glob('ThTaxSigns/data/output/exp/*.jpg'): \n    display(Image(filename=imageName))\n    print(\"\\n\")\n</code></pre>"},{"location":"get_started/#display-crop-output","title":"Display crop output","text":"<p>This tool will separatly crop the sign image output to several image files.</p> <pre><code># Display all crop images\n\nfor imageName in glob.glob('ThTaxSigns/data/output/exp/crops/unidentified_signs/*.jpg'): \n    display(Image(filename=imageName,width=300))\n    print(\"\\n\")\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Finally, you can see these crop image outputs on <code>output/exp/crops/unidentified_signs</code> folder.</p>"},{"location":"home/","title":"Home","text":""},{"location":"home/#what-is-thtaxsign","title":"What is ThTaxSign ?","text":"<p>ThTaxSign is a custom object detection model trained on the YOLOv5 architecture, specifically designed to detect road signs or municipal tax signs in Thailand. Leveraging the robustness and efficiency of YOLOv5, ThTaxSign excels at accurately identifying various types of road signs commonly found on Thai roads, including regulatory signs, warning signs, construction signs, guidepost signs, soi/road signs and unidentified signs.</p> <p>Trained on a diverse dataset consisting of annotated images of Thai road signs, ThTaxSign has learned to recognize and localize signs with high precision and reliability. This model is optimized to perform well under various environmental conditions, including different lighting conditions, weather conditions, and viewing angles commonly encountered on Thailand roads.</p> <p>The municipal tax sign detection in this model is categorized under \"unidentified signs\" due to the absence of distinct shapes or clearly defined colors. Municipal tax signs may resemble advertisements, store signs, or brand signs with specific colors and shapes, thus falling into this category. Users are required to further differentiate these signs based on visual cues after detection of unidentified signs.</p> <p></p>"},{"location":"home/#catagories-of-thtaxsign-model","title":"Catagories of ThTaxSign model","text":"<p>The signs of this model are divided into six categories, including construction signs, guide post signs, regulatory signs, soi/road signs, warning signs, and unidentified signs.</p> <ul> <li>Construction sign</li> </ul> <p></p> <ul> <li>Guide Post sign</li> </ul> <p></p> <ul> <li>Regulatory sign</li> </ul> <p></p> <ul> <li>Soi/Road sign</li> </ul> <p></p> <ul> <li>warning sign</li> </ul> <p></p> <ul> <li>unidentified sign</li> </ul> <p></p>"},{"location":"home/#the-purpose-of-this-project","title":"The purpose of this project","text":"<p>This project was developed under the GISTDA (Geo-Informatics and Space Technology Development Agency, Thailand ) initiative to detect various Municipal tax signs such as shop signs, advertisements, and other signs from street view images.</p> <p></p>"},{"location":"yolov5/","title":"YOLOv5","text":""},{"location":"yolov5/#what-is-yolov5","title":"What is YOLOv5","text":"<p>YOLOv5 is a prominent member of the YOLO (You Only Look Once) series of computer vision models developed by Ultralytics, widely employed for object detection tasks in various applications such as autonomous driving, surveillance, and image understanding. YOLOv5 distinguishes itself with its efficiency and accuracy, providing real-time object detection capabilities More. </p> <p> </p>"},{"location":"yolov5/#yolov5-architecture","title":"YOLOv5 Architecture","text":"<p>YOLOv5 Architecture</p> <p>Object detection, a use case for which YOLOv5 is designed, involves creating features from input images. These features are then fed through a prediction system to draw boxes around objects and predict their classes More. </p> <p></p>"},{"location":"yolov5/#the-benefits-and-potential-of-yolov5","title":"The benefits and potential of YOLOv5","text":""}]}